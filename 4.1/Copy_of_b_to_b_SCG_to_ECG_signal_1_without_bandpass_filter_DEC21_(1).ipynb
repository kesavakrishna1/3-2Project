{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kesavakrishna1/3-2Project/blob/main/4.1/Copy_of_b_to_b_SCG_to_ECG_signal_1_without_bandpass_filter_DEC21_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cem7L7F-6SiB"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_CFmfRUUMks",
        "outputId": "29774f79-ad0f-48d2-b494-0848cadfe6e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71TTGDer6Dko"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LayerNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpaGGeyzTWyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vkga0yxxTVid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW_tZJ9dIu-_",
        "outputId": "00e4672d-68bc-4987-a460-17eae4d9d154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVJfPhRdzJsT"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.python.client import device_lib\n",
        "# print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ummp5mKUOnWg",
        "outputId": "a083043b-38df-446b-d05e-84e73782640d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-najliuwg\n",
            "  Running command git clone --filter=blob:none --quiet https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-najliuwg\n",
            "  warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n",
            "  Resolved https://www.github.com/keras-team/keras-contrib.git to commit 3fc5ef709e061416f4bc8a92ca3750c824b5d2b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-contrib==2.0.8) (2.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtGrwIAvPScX",
        "outputId": "e5e6bcc3-d32c-4f86-e2c8-428493e21e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyedflib in /usr/local/lib/python3.10/dist-packages (0.1.36)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from pyedflib) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyedflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx3C2Z7jPs0P",
        "outputId": "0248b543-714f-44e5-c7c2-b25bb757c1aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: padasip in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from padasip) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install padasip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1VOk7-c5xF5"
      },
      "outputs": [],
      "source": [
        "fileNames = []\n",
        "fileName_str = []\n",
        "#Change the number here to read different files\n",
        "for i in range(10,11,1):\n",
        "  name_str = ''\n",
        "  if(i<10):\n",
        "    name_str = 'b00'\n",
        "  else:\n",
        "    name_str = 'b0'\n",
        "  fileNames.append(name_str+ str(i)+'.edf')\n",
        "  fileName_str.append(name_str+ str(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OjZGW0ABl60"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt, find_peaks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_TPhcggPIG5"
      },
      "outputs": [],
      "source": [
        "import pyedflib\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.preprocessing import scale\n",
        "import padasip as pa\n",
        "from scipy import signal\n",
        "from keras.layers import LeakyReLU\n",
        "original_scg = []\n",
        "original_ecg = []\n",
        "class DataUtils:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.fileNames = fileNames\n",
        "\n",
        "\n",
        "    # def readData(self, sigNum, path=\"/content/drive/MyDrive/Colab Notebooks/basal/\"):\n",
        "    def readData(self, sigNum, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        file_name = path + self.fileNames[sigNum]\n",
        "        #print(file_name)\n",
        "        #file_name =  self.fileNames[sigNum]\n",
        "        f = pyedflib.EdfReader(file_name)\n",
        "        n = f.signals_in_file\n",
        "        signal_labels = f.getSignalLabels()\n",
        "        print(\"Reading file:: \",file_name)\n",
        "        print(\"different columns:: \",signal_labels)\n",
        "        print(\"total number of samples\",f.getNSamples())\n",
        "        abdECG = np.zeros((1, f.getNSamples()[0]))\n",
        "        scg = np.zeros((1, f.getNSamples()[0]))\n",
        "        scg[0, :] = f.readSignal(3)\n",
        "        scg = scale(scg, axis=1)\n",
        "        #scg[0, :] = scale(self.butter_bandpass_filter(scg, 1, 200, 1000), axis=1)\n",
        "        #for i in np.arange(0, n-2):\n",
        "        abdECG[0, :] = f.readSignal(0)\n",
        "        abdECG = scale(abdECG, axis=1)\n",
        "        #abdECG = scale(self.butter_bandpass_filter(abdECG, 1, 110, 1000), axis=1)\n",
        "        print(\"before downsampling\",abdECG.shape[1])\n",
        "        abdECG = signal.resample(abdECG, int(abdECG.shape[1] / 5), axis=1)\n",
        "        scg = signal.resample(scg, int(scg.shape[1] / 5), axis=1)\n",
        "\n",
        "        # # differencing code starts\n",
        "        # scg = np.diff(scg)\n",
        "        # abdECG = np.diff(abdECG)\n",
        "        # # differencing code ends\n",
        "\n",
        "        print(\"after downsampling\",abdECG.shape[1])\n",
        "        '''function for ecg identification, using pan-tompkins algorithm'''\n",
        "        fs = []\n",
        "        fs.append(f.getSampleFrequency(0))\n",
        "\n",
        "        #changed the return sequence, so that converion will be from SCG to ECG\n",
        "        return  scg, abdECG, fs\n",
        "\n",
        "    def windowingSig(self, sig1, sig2, windowSize=15):\n",
        "        signalLen = sig2.shape[1]\n",
        "        signalsWindow1 = [sig1[:, int(i):int(i + windowSize)].transpose() for i in range(0, signalLen - windowSize, windowSize)]\n",
        "        signalsWindow2 = [sig2[:, int(i):int(i + windowSize)].transpose() for i in range(0, signalLen - windowSize, windowSize)]\n",
        "        print(\"SCG shape after windowing:: \",np.array(signalsWindow1).shape)\n",
        "        print(\"ECG shape after windowing:: \",np.array(signalsWindow2).shape)\n",
        "\n",
        "        return signalsWindow1, signalsWindow2\n",
        "\n",
        "    def adaptFilterOnSig(self, src, ref):\n",
        "        f = pa.filters.FilterNLMS(n=4, mu=0.1, w=\"random\")\n",
        "        for index, sig in enumerate(src):\n",
        "            try:\n",
        "                y, e, w = f.run(ref[index][:, 0], sig)\n",
        "                ref[index][:, 0] = e\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return ref\n",
        "\n",
        "    def calculateICA(self, sdSig, component=7):\n",
        "        ica = FastICA(n_components=component, max_iter=1000)\n",
        "        icaRes = []\n",
        "        for index, sig in enumerate(sdSig):\n",
        "            try:\n",
        "                icaSignal = np.array(ica.fit_transform(sig))\n",
        "                icaSignal = np.append(icaSignal, sig[:, range(2, 4)], axis=1)\n",
        "                icaRes.append(icaSignal)\n",
        "            except:\n",
        "                pass\n",
        "        return np.array(icaRes)\n",
        "\n",
        "    def createDelayRepetition(self, signal, numberDelay=4, delay=10):\n",
        "        signal = np.repeat(signal, numberDelay, axis=0)\n",
        "        #print(\"createDelayRepetition::\",signal.shape)\n",
        "        for row in range(1, signal.shape[0]):\n",
        "            signal[row, :] = np.roll(signal[row, :], shift=delay * row)\n",
        "\n",
        "        #print(\"createDelayRepetition 2::\",signal.shape)\n",
        "        return signal\n",
        "\n",
        "    def __butter_bandpass(self, lowcut, highcut, fs, order=5):\n",
        "        #print(\"00\")\n",
        "        nyq = 0.5 * fs\n",
        "        #print(\"0\")\n",
        "        low = lowcut / nyq\n",
        "        #print(\"11\")\n",
        "        high = highcut / nyq\n",
        "        b, a = butter(order, [low, high], btype='band')\n",
        "        #print(\"1\")\n",
        "        return b, a\n",
        "\n",
        "    def butter_bandpass_filter(self, data, lowcut, highcut, fs, order=3, axis=1):\n",
        "        #print(\"In butter_bandpass_filter\")\n",
        "        b, a = self.__butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        y = filtfilt(b, a, data, axis=axis)\n",
        "        #print(\"2\")\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMp0XeQhDxva"
      },
      "outputs": [],
      "source": [
        "'''function for ecg identification, using pan-tompkins algorithm'''\n",
        "\n",
        "def codeFn(ecg, fs):\n",
        "    nyquist = 0.5 * fs\n",
        "    low_cutoff = 5\n",
        "    high_cutoff = 15\n",
        "    b, a = butter(1, [low_cutoff/nyquist, high_cutoff/nyquist], btype='band')\n",
        "    ecg_filt = filtfilt(b, a, ecg)\n",
        "\n",
        "    b = np.array([1, 0, -1])\n",
        "    ecg_diff = np.convolve(ecg_filt, b, mode='same')\n",
        "    ecg_sq = ecg_diff ** 2\n",
        "\n",
        "    ma_len = int(0.08 * fs)\n",
        "    ecg_ma = np.convolve(ecg_sq, np.ones(ma_len)/ma_len, mode='same')\n",
        "\n",
        "    qrs_idx, _ = find_peaks(ecg_ma, distance=int(0.2 * fs), height=0.2 * np.max(ecg_ma))\n",
        "\n",
        "    return qrs_idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyDUBs4I4keJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#from Utils.DataUtils import DataUtils\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class TrainUtils:\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.dataUtils = DataUtils()\n",
        "\n",
        "    def prepareData(self, delay=5, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/b010.edf\"):\n",
        "        scgAll, ecg, fs = self.dataUtils.readData(0,path)\n",
        "        print(scgAll.shape)\n",
        "        #print(\"scg shape:: \",ecg.shape)\n",
        "        scgAll = scgAll[range(1), :]\n",
        "        #print(\"Number of samples:: \",scg.shape)\n",
        "        delayNum = scgAll.shape[0]\n",
        "        ecgAll = self.dataUtils.createDelayRepetition(ecg, delayNum, delay)\n",
        "        #print(\"scg all shape:: \",ecgAll.shape)\n",
        "        for i in range(1, len(fileNames)):\n",
        "            scg, ecg = self.dataUtils.readData(i,path)\n",
        "            print(\"Number of samples:: \",ecg.shape)\n",
        "            scg = scg[range(1), :]\n",
        "            ecgDelayed = self.dataUtils.createDelayRepetition(ecg, 1, delay)\n",
        "            scgAll = np.append(scgAll, scg, axis=1)\n",
        "            ecgAll = np.append(ecgAll, ecgDelayed, axis=1)\n",
        "        #print(\"SCG all merged shape:: \", scg.shape)\n",
        "        print(\"ECG all merged shape:: \", ecgAll.shape)\n",
        "\n",
        "        original_scg = scgAll\n",
        "        original_ecg = ecgAll\n",
        "        scgWindows, ecgWindows = self.dataUtils.windowingSig(scgAll, ecgAll, windowSize=1000)\n",
        "        # ecgWindows = self.dataUtils.adaptFilterOnSig(scgWindows, ecgWindows)\n",
        "        # scgWindows = self.dataUtils.calculateICA(scgWindows, component=2)\n",
        "        return scgWindows, ecgWindows\n",
        "\n",
        "    def trainTestSplit(self, sig, label, trainPercent, shuffle=True):\n",
        "        print(\"Splitting into train and test:: \")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(sig, label, train_size=trainPercent, shuffle=False)\n",
        "        X_train = np.array(X_train)\n",
        "        #X_train = self.window_the_data(X_train,  60)\n",
        "        X_test = np.array(X_test)\n",
        "        #X_test = self.window_the_data(X_test,  60)\n",
        "        y_train = np.array(y_train)\n",
        "        y_test = np.array(y_test)\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def window_the_data(self,data,window=32):\n",
        "      r = list()\n",
        "      for i, mat in enumerate(data):\n",
        "        s = 0\n",
        "        a = list()\n",
        "        print(data.shape)\n",
        "        while s + window < 256:\n",
        "          a.append(copy.deepcopy(mat[s : s + window].T))\n",
        "          # for i,x in enumerate(a):\n",
        "            # a[i] = scaler.fit_transform(x)\n",
        "          s += 2\n",
        "        r.append(copy.deepcopy(a))\n",
        "        #print(r.shape())\n",
        "\n",
        "\n",
        "      r = np.array(r)\n",
        "      return copy.deepcopy(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfyVIAEdz6iL"
      },
      "outputs": [],
      "source": [
        "# '''function for ecg identification, using pan-tompkins algorithm'''\n",
        "\n",
        "# def codeFn(ecg, fs):\n",
        "#     nyquist = 0.5 * fs\n",
        "#     low_cutoff = 5\n",
        "#     high_cutoff = 15\n",
        "#     b, a = butter(1, [low_cutoff/nyquist, high_cutoff/nyquist], btype='band')\n",
        "#     ecg_filt = filtfilt(b, a, ecg)\n",
        "\n",
        "#     b = np.array([1, 0, -1])\n",
        "#     ecg_diff = np.convolve(ecg_filt, b, mode='same')\n",
        "#     ecg_sq = ecg_diff ** 2\n",
        "\n",
        "#     ma_len = int(0.08 * fs)\n",
        "#     ecg_ma = np.convolve(ecg_sq, np.ones(ma_len)/ma_len, mode='same')\n",
        "\n",
        "#     qrs_idx, _ = find_peaks(ecg_ma, distance=int(0.2 * fs), height=0.2 * np.max(ecg_ma))\n",
        "\n",
        "#     return qrs_idx\n",
        "\n",
        "# # f=readData(,,)\n",
        "# fs = f.getSampleFrequency(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "S6hm8PqtUDsN",
        "outputId": "84d6dde3-ea1b-47db-ec62-18968b9a0897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Using cached keras-3.0.1-py3-none-any.whl (999 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.23.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0.post1 requires keras<2.16,>=2.15.0, but you have keras 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X2IeRM92QgC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "outputId": "8363510d-407f-41ef-9dff-7031fdcc2d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.0.1\n",
            "    Uninstalling keras-3.0.1:\n",
            "      Successfully uninstalled keras-3.0.1\n",
            "Successfully installed keras-2.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    **added 2 discriminators**\n",
        "\n",
        "---\n",
        "\n",
        "* line 103, 104 self.image shape c and d\n",
        "* line 124 - 140 discriminator c and d\n",
        "* line 151, 152 modified generators\n",
        "* line 167, 168 added fake_c and d\n",
        "* line 286 and 287 added batches C and D\n",
        "* line 299-302 added imgs_c, imgs_d\n",
        "* line 310, 311 fake_c and d\n",
        "* line 315, 316 added reconstr_c and d\n",
        "* line 324 added img_c and d for g_loss\n",
        "* line 326-332 added dC and dD loss real and fake\n",
        "* line 335 modified d_loss\n",
        "* line 376-384 added fake_c and d, reconstr_c and d\n",
        "* line 394 added imgs_C, fake_D, reconstr_C, imgs_D, fake_C, reconstr_D to gen_imgs\n"
      ],
      "metadata": {
        "id": "jWSGLeDSYotR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow\n",
        "!python -m pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrC8CwOO0hmw",
        "outputId": "3b58322b-3655-4b4f-a5f8-8a0b1791a72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Name: tensorflow\n",
            "Version: 2.15.0.post1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine-rl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ! pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQtYuofz2vLW",
        "outputId": "d132eea8-c4cb-4361-f533-a96459327e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  !python -m pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GxYpx5Q2_Pg",
        "outputId": "932a8cdc-340a-46e5-8273-8dbfafe50b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.15.0.post1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine-rl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!apt-get install python-pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnMnUUTM3lcX",
        "outputId": "1ec5066d-8e7e-4357-ddff-f9ee4076f6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python-pip is already the newest version (20.3.4+dfsg-4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dateutil\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR0X-w1i3_pf",
        "outputId": "9d8ad6be-d92e-4823-b8c7-eaa8c08d8461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-addons\n",
        "\n",
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "b2EGkYlUK0ph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2906f6-81e2-49cf-f228-29c58e8adeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "      # !pip install -U dateutil"
      ],
      "metadata": {
        "id": "f-YkzRx7LLZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow_addons.layers import InstanceNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam as legacy_optimizers\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "# from dateutil import Dateutils\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras import Input, Model\n",
        "from keras.layers import Conv1D, UpSampling1D, LeakyReLU, Dropout, Lambda, Embedding, Bidirectional, LSTM, Dense, Flatten, Layer, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import legacy as legacy_optimizers\n",
        "from keras_contrib.layers import InstanceNormalization\n",
        "#from keras_self_attention import ScaledDotProductAttention\n",
        "#from keras_self_attention.backend import regularizers\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import Sequential\n",
        "# from tensorflow.python.keras.layers import Normalization\n",
        "from tensorflow.keras.layers import Normalization\n",
        "\n",
        "#from Utils.DataUtils import DataUtils\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow_addons.layers import InstanceNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "!pip install tensorflow-addons\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam as legacy_optimizers\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "# from tensorflow.keras.losses import huber_loss\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "# from dateutils import Dateutils\n",
        "\n",
        "# class TransformerBlock(Layer):\n",
        "#     def _init_(self, embed_dim, num_heads, rate=0.1):\n",
        "#         super(TransformerBlock, self)._init_()\n",
        "#         self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "#         # self.ffn = Sequential(\n",
        "#         #     [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim), ]\n",
        "#         # )\n",
        "#         #print(\"apply layered normalization\")\n",
        "#         self.layernorm1 = LayerNormalization(epsilon=1e-4)\n",
        "#         # self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "#         #print(\"apply dropout\")\n",
        "#         self.dropout1 = Dropout(rate)\n",
        "#         # self.dropout2 = Dropout(rate)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         #print(\"In call function of transformer\")\n",
        "#         attn_output = self.att(inputs, inputs)\n",
        "#         attn_output = self.dropout1(attn_output)\n",
        "#         out1 = self.layernorm1(inputs * attn_output)\n",
        "#         # ffn_output = self.ffn(out1)\n",
        "#         # ffn_output = self.dropout2(ffn_output)\n",
        "#         return out1\n",
        "#     def get_config(self):\n",
        "#         \"\"\"\n",
        "#             For rebuilding models on load time.\n",
        "#         \"\"\"\n",
        "\n",
        "#         #config = {\n",
        "#         #    'output_dim': self.output_dim,\n",
        "#         #    'units': self.units,\n",
        "#         #    'return_probabilities': self.return_probabilities\n",
        "#         #}\n",
        "#         base_config = super(TransformerBlock, self).get_config()\n",
        "#         return dict(list(base_config.items()))\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        # self.ffn = Sequential(\n",
        "        #     [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim), ]\n",
        "        # )\n",
        "        #print(\"apply layered normalization\")\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-4)\n",
        "        # self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        #print(\"apply dropout\")\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        # self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #print(\"In call function of transformer\")\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs * attn_output)\n",
        "        # ffn_output = self.ffn(out1)\n",
        "        # ffn_output = self.dropout2(ffn_output)\n",
        "        return out1\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "\n",
        "        #config = {\n",
        "        #    'output_dim': self.output_dim,\n",
        "        #    'units': self.units,\n",
        "        #    'return_probabilities': self.return_probabilities\n",
        "        #}\n",
        "        base_config = super(TransformerBlock, self).get_config()\n",
        "        return dict(list(base_config.items()))\n",
        "\n",
        "\n",
        "\n",
        "def batch_creation(x_train, y_train, batch_size, batch_idx):\n",
        "    # Implement your batch creation logic here based on x_train and y_train\n",
        "    # Return two batches for source and target domains\n",
        "\n",
        "    batchA = x_train[batch_idx*batch_size:batch_idx*batch_size+batch_size]\n",
        "    batchB = y_train[batch_idx*batch_size:batch_idx*batch_size+batch_size]\n",
        "    batchC = y_train[batch_idx*batch_size:batch_idx*batch_size+batch_size]\n",
        "    batchD = y_train[batch_idx*batch_size:batch_idx*batch_size+batch_size]\n",
        "    # return batchA, batchB, batchC, batchD\n",
        "    return batchA, batchB, batchC, batchD\n",
        "\n",
        "\n",
        "class CycleGAN:\n",
        "    def __init__(self, row, col):\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                # Restrict TensorFlow to only use the fourth GPU\n",
        "                tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "                # Currently, memory growth needs to be the same across GPUs\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "            except RuntimeError as e:\n",
        "                # Memory growth must be set before GPUs have been initialized\n",
        "                print(e)\n",
        "        # Input shape\n",
        "        self.dataUtils = DataUtils()\n",
        "        self.img_rows = row\n",
        "        self.img_cols = col\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols)\n",
        "        self.img_shape_b = (self.img_rows,1)\n",
        "        self.img_shape_c = (self.img_rows,1)\n",
        "        self.img_shape_d = (self.img_rows,1)\n",
        "\n",
        "        # Configure data loader\n",
        "        self.dataset_name = 'ECG2FECG'\n",
        "\n",
        "        print(\"1\")\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        patch = int(self.img_rows / 2 ** 4)\n",
        "        self.disc_patch = (self.img_rows, 1)\n",
        "\n",
        "        # Number of filters in the first layer of G and D\n",
        "        self.gf = 6\n",
        "        self.df = 12\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cycle = 4.0  # Cycle-consistency loss\n",
        "        self.lambda_id = 0.01 * self.lambda_cycle  # Identity loss\n",
        "\n",
        "        # optimizer = Adam()\n",
        "        optimizer = tf.keras.optimizers.legacy.Adam()\n",
        "\n",
        "        # Build and compile the discriminators\n",
        "        self.d_A = self.build_discriminator(self.img_shape)\n",
        "        self.d_B = self.build_discriminator(self.img_shape_b)\n",
        "        self.d_C = self.build_discriminator(self.img_shape_c)\n",
        "        self.d_D = self.build_discriminator(self.img_shape_d)\n",
        "\n",
        "        # Build the generators\n",
        "        self.g_AB = self.build_generator(self.img_shape)\n",
        "        self.g_BA = self.build_generator(self.img_shape_b)\n",
        "\n",
        "        # Compile discriminators\n",
        "        self.d_A.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_B.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_C.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_D.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Input images from both domains\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape_b)\n",
        "        img_D = Input(shape=self.img_shape)\n",
        "        img_C = Input(shape=self.img_shape_b)\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB(img_A)\n",
        "        fake_A = self.g_BA(img_B)\n",
        "        fake_D = self.g_AB(img_B)\n",
        "        fake_C = self.g_BA(img_A)\n",
        "\n",
        "        # Translate images back to original domain\n",
        "        reconstr_A = self.g_BA(fake_B)\n",
        "        reconstr_B = self.g_AB(fake_A)\n",
        "        reconstr_C = self.g_BA(fake_D)\n",
        "        reconstr_D = self.g_AB(fake_C)\n",
        "        #print(\"1\")\n",
        "        # Identity mapping of images\n",
        "        img_A_id = self.g_BA(img_A)\n",
        "        img_B_id = self.g_AB(img_B)\n",
        "        img_D_id = self.g_BA(img_D)\n",
        "        img_C_id = self.g_AB(img_C)\n",
        "\n",
        "        #print(\"2\")\n",
        "        # For the combined model we will only train the generators\n",
        "        self.d_A.trainable = False\n",
        "        self.d_B.trainable = False\n",
        "        #print(\"3\")\n",
        "\n",
        "        # Discriminators determines validity of translated images\n",
        "        valid_A = self.d_A(fake_A)\n",
        "        valid_B = self.d_B(fake_B)\n",
        "        # valid_C = self.d_C.train_on_batch(fake_C, valid)\n",
        "        # valid_D = self.d_C.train_on_batch(fake_d, valid)\n",
        "        valid_C = self.d_C(fake_C)\n",
        "        valid_D = self.d_C(fake_D)\n",
        "        print(\"4\")\n",
        "\n",
        "        # Combined model trains generators to fool discriminators\n",
        "        self.combined = Model(inputs=[img_A, img_B, img_C, img_D],\n",
        "                              outputs=[valid_A, valid_B, valid_C, valid_D,\n",
        "                                       fake_B, fake_A, fake_D, fake_C,\n",
        "                                       reconstr_A, reconstr_B, reconstr_C, reconstr_D])\n",
        "        print(\"model created\")\n",
        "        # self.combined.compile(loss=['huber_loss', 'huber_loss',\n",
        "        #                             'huber_loss', 'huber_loss', 'huber_loss',\n",
        "        #                             'huber_loss'],\n",
        "        #                       loss_weights=[1, 1,\n",
        "        #                                     self.lambda_cycle, self.lambda_cycle,\n",
        "        #                                     self.lambda_id, self.lambda_id],\n",
        "        #                       optimizer=optimizer)\n",
        "\n",
        "        self.combined.compile(loss=[Huber(), Huber(),\n",
        "                            Huber(), Huber(), Huber(),\n",
        "                            Huber()],\n",
        "                      loss_weights=[1, 1,\n",
        "                                    self.lambda_cycle, self.lambda_cycle,\n",
        "                                    self.lambda_id, self.lambda_id],\n",
        "                      optimizer=optimizer)\n",
        "\n",
        "        print(\"Model compiled\")\n",
        "\n",
        "\n",
        "    # Define custom loss\n",
        "    def custom_loss(self):\n",
        "\n",
        "        # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
        "        def loss(y_true, y_pred):\n",
        "            return K.mean(y_true * K.log(y_true / y_pred + K.epsilon()))\n",
        "\n",
        "        # Return a function\n",
        "        return loss\n",
        "\n",
        "    def build_generator(self,img_shape):\n",
        "        \"\"\"U-Net Generator\"\"\"\n",
        "        print(\"In build generator\")\n",
        "        def conv1DWithSINE(layer_input, filters, f_size=30):\n",
        "            \"\"\"Layers used during downsampling\"\"\"\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same', activation='sigmoid')(layer_input)\n",
        "            d = InstanceNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        def multiply(x):\n",
        "            mask,image  = x\n",
        "            return image* K.clip(mask,0.8,1)\n",
        "\n",
        "        input = Input(shape=img_shape)\n",
        "        print(\"Input shape:: \", input.shape)\n",
        "        value = conv1DWithSINE(input, input.shape[2], f_size=30)\n",
        "        print(\" shape after conv1d :: \", value.shape)\n",
        "\n",
        "        print(\"apply attention.\")\n",
        "        att = TransformerBlock(embed_dim=input.shape[1], num_heads=2)(value)\n",
        "        print(\"shape after attention:: \",att.shape )\n",
        "        att = Normalization(axis=1)(att)\n",
        "        print(\"shape after normalizing attention:: \",att.shape )\n",
        "\n",
        "        remainedInput = Lambda(multiply)([att, value])\n",
        "        print(\"shape after apply lambda:: \",remainedInput.shape )\n",
        "\n",
        "        output_img = conv1DWithSINE(remainedInput, 13, f_size=13)\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=13)\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=13)\n",
        "        output_img = conv1DWithSINE(output_img, 1, f_size=1)\n",
        "\n",
        "        return Model(input, output_img)\n",
        "\n",
        "    def build_discriminator(self,img_shape):\n",
        "        print(\"In building discriminator\")\n",
        "        def d_layer(layer_input, filters, f_size=13, normalization=True):\n",
        "            \"\"\"Discriminator layer\"\"\"\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same',activation='sigmoid')(layer_input)\n",
        "            if normalization:\n",
        "                d = InstanceNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        img = Input(shape=img_shape)\n",
        "        print(\"input shape:: \",img.shape)\n",
        "        d1 = d_layer(img, self.df)\n",
        "        print(\"input shape after first conv1d:: \",d1.shape)\n",
        "        d2 = d_layer(d1, 7)\n",
        "        print(\"input shape after second conv1d:: \",d2.shape)\n",
        "        d3 = d_layer(d2, 5)\n",
        "        print(\"input shape after third conv1d:: \",d3.shape)\n",
        "        validity = d_layer(d3, 3)\n",
        "        # validity = d_layer(d3, img.shape[1])\n",
        "        print(\"final shape after 4th layer:: \",validity.shape)\n",
        "\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, x_train, y_train, epochs, batch_size=10, sample_interval=20):\n",
        "        print(\"Num Samples\", x_train.shape[0])\n",
        "        print(\"In train:: \")\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        # Adversarial loss ground truths\n",
        "        valid = np.ones((batch_size,) + self.disc_patch)\n",
        "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_idx in range(x_train.shape[0] // batch_size):\n",
        "                batchA, batchB, batchC, batchD = batch_creation(x_train, y_train, batch_size, batch_idx)\n",
        "                batchA = np.asarray(batchA)\n",
        "                batchB = np.asarray(batchB)\n",
        "                batchC = np.asarray(batchC)\n",
        "                batchD = np.asarray(batchD)\n",
        "\n",
        "                imgs_A = batchA\n",
        "                imgs_B = batchB\n",
        "                imgs_C = batchC\n",
        "                imgs_D = batchD\n",
        "                imgs_B = np.reshape(imgs_B, (-1, x_train.shape[1], x_train.shape[2]))\n",
        "                imgs_A = np.reshape(imgs_A, (-1, x_train.shape[1], x_train.shape[2]))\n",
        "                imgs_D = np.reshape(imgs_D, (-1, x_train.shape[1], x_train.shape[2]))\n",
        "                imgs_C = np.reshape(imgs_C, (-1, x_train.shape[1], x_train.shape[2]))\n",
        "                # ----------------------\n",
        "                #  Train Discriminators\n",
        "                # ----------------------\n",
        "\n",
        "                # Translate images to the other domain\n",
        "                fake_B = self.g_AB.predict(imgs_A)\n",
        "                fake_A = self.g_BA.predict(imgs_B)\n",
        "                fake_D = self.g_AB.predict(imgs_C)\n",
        "                fake_C = self.g_BA.predict(imgs_D)\n",
        "\n",
        "                # Translate images back to original domain\n",
        "                reconstr_A = self.g_BA.predict(fake_B)\n",
        "                reconstr_B = self.g_AB.predict(fake_A)\n",
        "                reconstr_C = self.g_BA.predict(fake_D)\n",
        "                reconstr_D = self.g_AB.predict(fake_C)\n",
        "\n",
        "                # Train the discriminators (original images = real / translated = Fake)\n",
        "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
        "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
        "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "\n",
        "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
        "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
        "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "                dC_loss_real = self.d_C.train_on_batch(imgs_C, valid)\n",
        "                dC_loss_fake = self.d_C.train_on_batch(fake_C, fake)\n",
        "                dC_loss = 0.5 * np.add(dC_loss_real, dC_loss_fake)\n",
        "\n",
        "                dD_loss_real = self.d_D.train_on_batch(imgs_D, valid)\n",
        "                dD_loss_fake = self.d_D.train_on_batch(fake_D, fake)\n",
        "                dD_loss = 0.5 * np.add(dD_loss_real, dD_loss_fake)\n",
        "\n",
        "                # Total discriminator loss\n",
        "                # d_loss = 0.25 * np.add(dA_loss, dB_loss, dC_loss, dD_loss)\n",
        "                # Total discriminator loss\n",
        "                d_loss = 0.25 * (dA_loss + dB_loss + dC_loss + dD_loss)\n",
        "\n",
        "\n",
        "                # ------------------\n",
        "                #  Train Generators\n",
        "                # ------------------\n",
        "\n",
        "                # Train the generators\n",
        "\n",
        "                # g_loss = self.combined.train_on_batch([imgs_A, imgs_B, imgs_C, imgs_D],\n",
        "                #                                       [valid, valid, valid, valid,\n",
        "                #                                        imgs_B, imgs_A, imgs_D, imgs_C,\n",
        "                #                                        reconstr_A, reconstr_B,\n",
        "                #                                        reconstr_C, reconstr_D])\n",
        "                # g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
        "                #                                       [valid, valid,\n",
        "                #                                        imgs_B, imgs_A,\n",
        "                #                                        reconstr_A, reconstr_B])\n",
        "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B, imgs_C, imgs_D],\n",
        "                                      [valid, valid, valid, valid,\n",
        "                                       imgs_B, imgs_A, imgs_D, imgs_C,\n",
        "                                       reconstr_A, reconstr_B,\n",
        "                                       reconstr_C, reconstr_D])\n",
        "\n",
        "\n",
        "                # # Translate images to the other domain\n",
        "                # fake_B = self.g_AB.predict(imgs_A)\n",
        "                # fake_A = self.g_BA.predict(imgs_B)\n",
        "                # # Translate images back to original domain\n",
        "                # reconstr_A = self.g_BA.predict(fake_B)\n",
        "                # reconstr_B = self.g_AB.predict(fake_A)\n",
        "                # # Train the discriminators (original images = real / translated = Fake)\n",
        "                # dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
        "                # dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
        "                # dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "\n",
        "                # dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
        "                # dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
        "                # dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "                # # Total disciminator loss\n",
        "                # d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "\n",
        "                # # ------------------\n",
        "                # #  Train Generators\n",
        "                # # ------------------\n",
        "\n",
        "\n",
        "                # # Train the generators\n",
        "                # g_loss = self.combined.train_on_batch([imgs_A, imgs_B, imgs_C, imgs_D],\n",
        "                #                                       [valid, valid,\n",
        "                #                                        imgs_B, imgs_A,\n",
        "                #                                        reconstr_A, reconstr_B,\n",
        "                #                                        valid_C, valid_D])\n",
        "\n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "\n",
        "                # Plot the progress\n",
        "                print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
        "                      % (epoch, epochs,\n",
        "                         batch_idx, 1,\n",
        "                         d_loss[0], 100 * d_loss[1],\n",
        "                         g_loss[0],\n",
        "                         np.mean(g_loss[1:3]),\n",
        "                         np.mean(g_loss[3:5]),\n",
        "                         np.mean(g_loss[5:6]),\n",
        "                         elapsed_time))\n",
        "\n",
        "                # If at save interval => save generated image samples\n",
        "                #print(\"shape of image a\",imgs_A.shape)\n",
        "                #print(\"shape of image b\",imgs_B.shape)\n",
        "                if batch_idx % sample_interval == 0:\n",
        "                    self.sample_images(epoch, batch_idx, imgs_A, imgs_B, imgs_C, imgs_D)\n",
        "                    self.g_AB.save(\"ECG2FECG.h5\", overwrite=True)\n",
        "                    self.g_BA.save(\"FECG2ECG.h5\", overwrite=True)\n",
        "\n",
        "    def sample_images(self, epoch, batch_idx, imgs_A, imgs_B, imgs_C, imgs_D):\n",
        "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "        r, c = 2, 3\n",
        "\n",
        "        # Demo (for GIF)\n",
        "        # imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
        "        # imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB.predict(imgs_A)\n",
        "        fake_A = self.g_BA.predict(imgs_B)\n",
        "        fake_D = self.g_AB.predict(imgs_B)\n",
        "        fake_C = self.g_BA.predict(imgs_A)\n",
        "        # Translate back to original domain\n",
        "        reconstr_A = self.g_BA.predict(fake_B)\n",
        "        reconstr_B = self.g_AB.predict(fake_A)\n",
        "        reconstr_C = self.g_BA.predict(fake_D)\n",
        "        reconstr_D = self.g_AB.predict(fake_C)\n",
        "\n",
        "\n",
        "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B, imgs_C, fake_D, reconstr_C, imgs_D, fake_C, reconstr_D])\n",
        "        #print(\"shape of gen\",gen_imgs.shape)\n",
        "        # Rescale images 0 - 1\n",
        "        # gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Translated', 'Reconstructed']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "\n",
        "        # gen_imgs[1] = gen_imgs[0]-gen_imgs[1]\n",
        "        try:\n",
        "            for i in range(r):\n",
        "                for j in range(c):\n",
        "                    for bias in range(1):\n",
        "                        #gen_imgs[cnt][:, bias] = scale(self.dataUtils.butter_bandpass_filter(gen_imgs[cnt][:, bias], 10, 50, 200, axis=0), axis=0)\n",
        "                        #gen_imgs[cnt][:, bias] = scale(gen_imgs[cnt][:, bias], axis=0)\n",
        "                        if np.max(gen_imgs[cnt][:, bias]) != 0:\n",
        "                            gen_imgs[cnt][:, bias] = gen_imgs[cnt][:, bias] / np.max(gen_imgs[cnt][:, bias])\n",
        "                        axs[i, j].plot(gen_imgs[cnt][:, bias] + bias)\n",
        "                    axs[i, j].set_title(titles[j])\n",
        "                    cnt += 1\n",
        "            # fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_idx))\n",
        "            plt.close()\n",
        "        except:\n",
        "            pass\n",
        "\n"
      ],
      "metadata": {
        "id": "37dDUUE8fiBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2740c5fa-eb2d-42ec-b508-a62ccb2288e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NT-yp5rl3MXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWEa6jqtQECU"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "from keras.layers import TimeDistributed\n",
        "#from keras.layers.recurrent import Recurrent\n",
        "from tensorflow.keras.layers import InputSpec\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AttentionDecoder():\n",
        "\n",
        "    def __init__(self, units, output_dim,\n",
        "                 activation='tanh',\n",
        "                 return_probabilities=False,\n",
        "                 name='AttentionDecoder',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
        "        encoder and outputs the decoded states\n",
        "        :param units: dimension of the hidden state and the attention matrices\n",
        "        :param output_dim: the number of labels in the output space\n",
        "        references:\n",
        "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
        "            \"Neural machine translation by jointly learning to align and translate.\"\n",
        "            arXiv preprint arXiv:1409.0473 (2014).\n",
        "        \"\"\"\n",
        "        self.units = units\n",
        "        self.output_dim = output_dim\n",
        "        self.return_probabilities = return_probabilities\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\n",
        "        self.name = name\n",
        "        self.return_sequences = True  # must return sequences\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
        "          for model details that correspond to the matrices here.\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
        "\n",
        "        if self.stateful:\n",
        "            super(AttentionDecoder, self).reset_states()\n",
        "\n",
        "        self.states = [None, None]  # y, s\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for creating the context vector\n",
        "        \"\"\"\n",
        "\n",
        "        self.V_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='V_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='W_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='U_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.b_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_a',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the r (reset) gate\n",
        "        \"\"\"\n",
        "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_r = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_r',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for the z (update) gate\n",
        "        \"\"\"\n",
        "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_z = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_z',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the proposal\n",
        "        \"\"\"\n",
        "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_p = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_p',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for making the final prediction vector\n",
        "        \"\"\"\n",
        "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                   name='C_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
        "                                   name='U_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
        "                                   name='W_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_o = self.add_weight(shape=(self.output_dim,),\n",
        "                                   name='b_o',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        # For creating the initial state:\n",
        "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='W_s',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "\n",
        "        self.input_spec = [\n",
        "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
        "        self.built = True\n",
        "\n",
        "    def _time_distributed_dense(self,x, w, b=None, dropout=None,\n",
        "                                input_dim=None, output_dim=None,\n",
        "                                timesteps=None, training=None):\n",
        "        \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
        "        # Arguments\n",
        "            x: input tensor.\n",
        "            w: weight matrix.\n",
        "            b: optional bias vector.\n",
        "            dropout: wether to apply dropout (same dropout mask\n",
        "                for every temporal slice of the input).\n",
        "            input_dim: integer; optional dimensionality of the input.\n",
        "            output_dim: integer; optional dimensionality of the output.\n",
        "            timesteps: integer; optional number of timesteps.\n",
        "            training: training phase tensor or boolean.\n",
        "        # Returns\n",
        "            Output tensor.\n",
        "        \"\"\"\n",
        "        if not input_dim:\n",
        "            input_dim = K.shape(x)[2]\n",
        "        if not timesteps:\n",
        "            timesteps = K.shape(x)[1]\n",
        "        if not output_dim:\n",
        "            output_dim = K.shape(w)[1]\n",
        "\n",
        "        if dropout is not None and 0. < dropout < 1.:\n",
        "            # apply the same dropout pattern at every timestep\n",
        "            ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
        "            dropout_matrix = K.dropout(ones, dropout)\n",
        "            expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
        "            x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
        "\n",
        "        # collapse time dimension and batch dimension together\n",
        "        x = K.reshape(x, (-1, input_dim))\n",
        "        x = K.dot(x, w)\n",
        "        if b is not None:\n",
        "            x = K.bias_add(x, b)\n",
        "        # reshape to 3D tensor\n",
        "        if K.backend() == 'tensorflow':\n",
        "            x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
        "            x.set_shape([None, None, output_dim])\n",
        "        else:\n",
        "            x = K.reshape(x, (-1, timesteps, output_dim))\n",
        "        return x\n",
        "\n",
        "    def call(self, x):\n",
        "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
        "        self.x_seq = x\n",
        "\n",
        "        # apply the a dense layer over the time dimension of the sequence\n",
        "        # do it here because it doesn't depend on any previous steps\n",
        "        # thefore we can save computation time:\n",
        "        self._uxpb = self._time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
        "                                             input_dim=self.input_dim,\n",
        "                                             timesteps=self.timesteps,\n",
        "                                             output_dim=self.units)\n",
        "\n",
        "        return super(AttentionDecoder, self).call(x, )\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        # apply the matrix on the first time step to get the initial s0.\n",
        "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
        "\n",
        "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
        "        # output_dim)\n",
        "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
        "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
        "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
        "        y0 = K.tile(y0, [1, self.output_dim])\n",
        "\n",
        "        return [y0, s0]\n",
        "\n",
        "    def step(self, x, states):\n",
        "\n",
        "        ytm, stm = states\n",
        "\n",
        "        # repeat the hidden state to the length of the sequence\n",
        "        _stm = K.repeat(stm, self.timesteps)\n",
        "\n",
        "        # now multiplty the weight matrix with the repeated hidden state\n",
        "        _Wxstm = K.dot(_stm, self.W_a)\n",
        "\n",
        "        # calculate the attention probabilities\n",
        "        # this relates how much other timesteps contributed to this one.\n",
        "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
        "                   K.expand_dims(self.V_a))\n",
        "        at = K.exp(et)\n",
        "        at_sum = K.sum(at, axis=1)\n",
        "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
        "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
        "\n",
        "        # calculate the context vector\n",
        "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
        "        # ~~~> calculate new hidden state\n",
        "        # first calculate the \"r\" gate:\n",
        "\n",
        "        rt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_r)\n",
        "            + K.dot(stm, self.U_r)\n",
        "            + K.dot(context, self.C_r)\n",
        "            + self.b_r)\n",
        "\n",
        "        # now calculate the \"z\" gate\n",
        "        zt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_z)\n",
        "            + K.dot(stm, self.U_z)\n",
        "            + K.dot(context, self.C_z)\n",
        "            + self.b_z)\n",
        "\n",
        "        # calculate the proposal hidden state:\n",
        "        s_tp = activations.tanh(\n",
        "            K.dot(ytm, self.W_p)\n",
        "            + K.dot((rt * stm), self.U_p)\n",
        "            + K.dot(context, self.C_p)\n",
        "            + self.b_p)\n",
        "\n",
        "        # new hidden state:\n",
        "        st = (1 - zt) * stm + zt * s_tp\n",
        "\n",
        "        yt = activations.softmax(\n",
        "            K.dot(ytm, self.W_o)\n",
        "            + K.dot(stm, self.U_o)\n",
        "            + K.dot(context, self.C_o)\n",
        "            + self.b_o)\n",
        "\n",
        "        if self.return_probabilities:\n",
        "            return at, [yt, st]\n",
        "        else:\n",
        "            return yt, [yt, st]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "            For Keras internal compatability checking\n",
        "        \"\"\"\n",
        "        if self.return_probabilities:\n",
        "            return (None, self.timesteps, self.timesteps)\n",
        "        else:\n",
        "            return (None, self.timesteps, self.output_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'units': self.units,\n",
        "            'return_probabilities': self.return_probabilities\n",
        "        }\n",
        "        base_config = super(AttentionDecoder, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "id": "pJftD9WFFW4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "\n",
        "# # See https://github.com/keras-rl/keras-rl/blob/master/rl/memory.py\n",
        "# def zeros_like(a, dtype='float32'):\n",
        "#     \"\"\"Return an array of zeros with same shape as given array.\n",
        "\n",
        "#     Args:\n",
        "#         a (array_like, iterable): An object with shape attribute or an iterable.\n",
        "\n",
        "#     Returns:\n",
        "#         (array_like, list): Array of zeros with the same shape as a.\n",
        "#     \"\"\"\n",
        "#     if hasattr(a, 'shape'):\n",
        "#         if hasattr(a, 'dtype'):\n",
        "#             dtype = a.dtype\n",
        "#         return np.zeros(a.shape, dtype=dtype)\n",
        "#     if hasattr(a, '__iter__'):\n",
        "#         return [zeros_like(b, dtype=dtype) for b in a]\n",
        "#     return 0.\n",
        "\n",
        "\n",
        "\n",
        "# def check_shape(a, b):\n",
        "#     \"\"\"Check if the shapes of given values match.\n",
        "\n",
        "#     Args:\n",
        "#         a (array_like, tuple): An object with shape attribute or a tuple representing shape.\n",
        "#         b (array_like, tuple): An object with shape attribute or a tuple representing shape.\n",
        "\n",
        "#     Raises:\n",
        "#         Exception: When shapes don't match.\n",
        "#     \"\"\"\n",
        "#     if hasattr(a, 'shape'):\n",
        "#         a = a.shape\n",
        "#     if hasattr(b, 'shape'):\n",
        "#         b = b.shape\n",
        "#     assert a == b, f\"Shapes {a} and {b} don't match\"\n",
        "\n",
        "\n",
        "\n",
        "# def unique(a):\n",
        "#     res = {id(v): v for v in a}\n",
        "#     return list(res.values())\n"
      ],
      "metadata": {
        "id": "z-bnbz6NG6rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXdJaaSpNkfO"
      },
      "outputs": [],
      "source": [
        "\n",
        "from unittest import TestCase\n",
        "import numpy as np\n",
        "\n",
        "# from utils.TrainUtils import TrainUtil\n",
        "# from utils.TrainUtils import TrainUtils\n",
        "# from deeplearning.CycleGAN import CycleGAN\n",
        "\n",
        "\n",
        "class TestCycleGAN(TestCase):\n",
        "\n",
        "    def __init__(self, methodName: str = ...) -> None:\n",
        "        super().__init__(methodName)\n",
        "        self.trainUtils = TrainUtils()\n",
        "\n",
        "    def test_trainSignal(self,path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001 (1)/b010.edf\"):\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2,path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001 (1)/\")\n",
        "        print(\"ecgWindows:: \",np.array(ecgWindows).shape)\n",
        "        X_train, X_test, Y_train, y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        X_train = np.reshape(X_train, [-1, X_train.shape[1], X_train.shape[2]])\n",
        "        # X_test = np.reshape(X_test, [-1, X_test.shape[1], X_test.shape[2], 1])\n",
        "        Y_train = np.reshape(Y_train, [-1, Y_train.shape[1], Y_train.shape[2]])\n",
        "\n",
        "        print(\"Shape of x train and y train:: \",X_train.shape, Y_train.shape)\n",
        "        print(\"Shape of x test and y test:: \",X_test.shape, y_test.shape)\n",
        "        # y_test = np.reshape(Y_test, [-1, Y_test.shape[1], Y_test.shape[2], 1])\n",
        "        cycleGAN = CycleGAN(X_train.shape[1], X_train.shape[2])\n",
        "        print(\"model instantiated\")\n",
        "        cycleGAN.train(x_train=X_train, y_train=Y_train, epochs=1)\n",
        "        return cycleGAN, X_test, y_test\n",
        "    def divide_test_train(self,path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001 (1)/b010.edf\"):\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2,path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001 (1)\")\n",
        "        X_train, X_test, Y_train, y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        return X_train, X_test, Y_train, y_test\n",
        "#model = TestCycleGAN(\"test_trainSignal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOqLtJow4Sbk"
      },
      "outputs": [],
      "source": [
        "model = TestCycleGAN(\"test_trainSignal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nk72Kbz7AxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc05374-6744-429d-9b8c-a51aab444c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file::  /content/drive/MyDrive/drive-download-20230428T081947Z-001 (1)/b010.edf\n",
            "different columns::  ['I', 'II', 'RESP', 'SCG']\n",
            "total number of samples [1505000 1505000 1505000 1505000]\n",
            "before downsampling 1505000\n",
            "after downsampling 301000\n",
            "(1, 301000)\n",
            "ECG all merged shape::  (1, 301000)\n",
            "SCG shape after windowing::  (300, 1000, 1)\n",
            "ECG shape after windowing::  (300, 1000, 1)\n",
            "ecgWindows::  (300, 1000, 1)\n",
            "Splitting into train and test:: \n",
            "Shape of x train and y train::  (225, 1000, 1) (225, 1000, 1)\n",
            "Shape of x test and y test::  (75, 1000, 1) (75, 1000, 1)\n",
            "1 Physical GPUs, 1 Logical GPUs\n",
            "1\n",
            "In building discriminator\n",
            "input shape::  (None, 1000, 1)\n",
            "input shape after first conv1d::  (None, 1000, 12)\n",
            "input shape after second conv1d::  (None, 1000, 7)\n",
            "input shape after third conv1d::  (None, 1000, 5)\n",
            "final shape after 4th layer::  (None, 1000, 3)\n",
            "In building discriminator\n",
            "input shape::  (None, 1000, 1)\n",
            "input shape after first conv1d::  (None, 1000, 12)\n",
            "input shape after second conv1d::  (None, 1000, 7)\n",
            "input shape after third conv1d::  (None, 1000, 5)\n",
            "final shape after 4th layer::  (None, 1000, 3)\n",
            "In building discriminator\n",
            "input shape::  (None, 1000, 1)\n",
            "input shape after first conv1d::  (None, 1000, 12)\n",
            "input shape after second conv1d::  (None, 1000, 7)\n",
            "input shape after third conv1d::  (None, 1000, 5)\n",
            "final shape after 4th layer::  (None, 1000, 3)\n",
            "In building discriminator\n",
            "input shape::  (None, 1000, 1)\n",
            "input shape after first conv1d::  (None, 1000, 12)\n",
            "input shape after second conv1d::  (None, 1000, 7)\n",
            "input shape after third conv1d::  (None, 1000, 5)\n",
            "final shape after 4th layer::  (None, 1000, 3)\n",
            "In build generator\n",
            "Input shape::  (None, 1000, 1)\n",
            " shape after conv1d ::  (None, 1000, 1)\n",
            "apply attention.\n",
            "shape after attention::  (None, 1000, 1)\n",
            "shape after normalizing attention::  (None, 1000, 1)\n",
            "shape after apply lambda::  (None, 1000, 1)\n",
            "In build generator\n",
            "Input shape::  (None, 1000, 1)\n",
            " shape after conv1d ::  (None, 1000, 1)\n",
            "apply attention.\n",
            "shape after attention::  (None, 1000, 1)\n",
            "shape after normalizing attention::  (None, 1000, 1)\n",
            "shape after apply lambda::  (None, 1000, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7b607cc84f70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "model created\n",
            "Model compiled\n",
            "model instantiated\n",
            "Num Samples 225\n",
            "In train:: \n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7b607cc84f70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1/1 [==============================] - 1s 627ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7b607d794af0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7b607d794af0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1/1 [==============================] - 1s 563ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b607cc57f40> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b607cc57f40> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b607d974040> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b607d974040> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b6075d24280> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b6075d24280> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b6075b01090> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b6075b01090> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b6075ada680> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7b6075ada680> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "[Epoch 0/1] [Batch 0/1] [D loss: 1.485881, acc:  31%] [G loss: 7.431767, adv: 0.734807, recon: 0.738422, id: 0.660117] time: 0:00:35.335130 \n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  `tf.saved_model.SaveOptions` object that specifies SavedModel\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "[Epoch 0/1] [Batch 1/1] [D loss: 1.475348, acc:  35%] [G loss: 7.295848, adv: 0.733102, recon: 0.722163, id: 0.606764] time: 0:00:37.307636 \n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "[Epoch 0/1] [Batch 2/1] [D loss: 1.462747, acc:  44%] [G loss: 7.133142, adv: 0.749021, recon: 0.697957, id: 0.584619] time: 0:00:38.421275 \n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "[Epoch 0/1] [Batch 3/1] [D loss: 1.448382, acc:  41%] [G loss: 7.064125, adv: 0.725170, recon: 0.695209, id: 0.563141] time: 0:00:39.546734 \n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "[Epoch 0/1] [Batch 4/1] [D loss: 1.442062, acc:  39%] [G loss: 7.062830, adv: 0.711589, recon: 0.698444, id: 0.555235] time: 0:00:40.646249 \n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "[Epoch 0/1] [Batch 5/1] [D loss: 1.436016, acc:  37%] [G loss: 7.095674, adv: 0.718971, recon: 0.700845, id: 0.540625] time: 0:00:41.770045 \n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "[Epoch 0/1] [Batch 6/1] [D loss: 1.426914, acc:  37%] [G loss: 7.017735, adv: 0.708941, recon: 0.693483, id: 0.554873] time: 0:00:43.160766 \n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "[Epoch 0/1] [Batch 7/1] [D loss: 1.413805, acc:  28%] [G loss: 6.934686, adv: 0.710562, recon: 0.682709, id: 0.559061] time: 0:00:44.891740 \n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "[Epoch 0/1] [Batch 8/1] [D loss: 1.404479, acc:  26%] [G loss: 6.815837, adv: 0.699316, recon: 0.670658, id: 0.583644] time: 0:00:46.555152 \n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "[Epoch 0/1] [Batch 9/1] [D loss: 1.387144, acc:  32%] [G loss: 6.785524, adv: 0.696094, recon: 0.667675, id: 0.583079] time: 0:00:48.283980 \n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "[Epoch 0/1] [Batch 10/1] [D loss: 1.376590, acc:  30%] [G loss: 6.677665, adv: 0.695012, recon: 0.654334, id: 0.580553] time: 0:00:49.405987 \n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "[Epoch 0/1] [Batch 11/1] [D loss: 1.361116, acc:  25%] [G loss: 6.529164, adv: 0.679429, recon: 0.639616, id: 0.576166] time: 0:00:50.554346 \n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "[Epoch 0/1] [Batch 12/1] [D loss: 1.344210, acc:  27%] [G loss: 6.452451, adv: 0.688167, recon: 0.628022, id: 0.566380] time: 0:00:51.669761 \n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "[Epoch 0/1] [Batch 13/1] [D loss: 1.326570, acc:  47%] [G loss: 6.435637, adv: 0.695631, recon: 0.624081, id: 0.575632] time: 0:00:52.771871 \n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "[Epoch 0/1] [Batch 14/1] [D loss: 1.313308, acc:  28%] [G loss: 6.576294, adv: 0.702827, recon: 0.639804, id: 0.566404] time: 0:00:53.867920 \n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "[Epoch 0/1] [Batch 15/1] [D loss: 1.307365, acc:  38%] [G loss: 6.406670, adv: 0.697845, recon: 0.619873, id: 0.572300] time: 0:00:54.959618 \n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "[Epoch 0/1] [Batch 16/1] [D loss: 1.293034, acc:  38%] [G loss: 6.233938, adv: 0.676011, recon: 0.603804, id: 0.556634] time: 0:00:56.078063 \n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "[Epoch 0/1] [Batch 17/1] [D loss: 1.274534, acc:  38%] [G loss: 6.141722, adv: 0.668432, recon: 0.594178, id: 0.556025] time: 0:00:57.170716 \n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "[Epoch 0/1] [Batch 18/1] [D loss: 1.268792, acc:  42%] [G loss: 6.016759, adv: 0.669093, recon: 0.578060, id: 0.585923] time: 0:00:58.270615 \n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "[Epoch 0/1] [Batch 19/1] [D loss: 1.252067, acc:  31%] [G loss: 5.999819, adv: 0.655311, recon: 0.579893, id: 0.539786] time: 0:00:59.673403 \n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "[Epoch 0/1] [Batch 20/1] [D loss: 1.241647, acc:  33%] [G loss: 5.870102, adv: 0.654435, recon: 0.563779, id: 0.555268] time: 0:01:01.336266 \n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "[Epoch 0/1] [Batch 21/1] [D loss: 1.232829, acc:  37%] [G loss: 5.752021, adv: 0.625921, recon: 0.556291, id: 0.571511] time: 0:01:03.867649 \n"
          ]
        }
      ],
      "source": [
        "cycleGan, X_test, y_test = model.test_trainSignal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg9qXAupK8yP"
      },
      "outputs": [],
      "source": [
        "#model = TestcycleGAN(\"test_trainSignal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NFoObzJ9ZxH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6if5-W4ALCFs"
      },
      "outputs": [],
      "source": [
        "#cycleGan, X_test, y_test = model.test_trainSignal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_EyygfkiS9D"
      },
      "outputs": [],
      "source": [
        "x_g_AB =cycleGan.g_AB.predict(X_test)\n",
        "#x_g_BA = cycleGan.g_BA.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOoeJ1miu81V"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# a = np.array(y_test.flatten())\n",
        "# b = x_g_AB.flatten()\n",
        "# print(len(a), len(b))\n",
        "# df1 = pd.DataFrame({ \"ECG Original\" : a})\n",
        "# df2 = pd.DataFrame({ \"ECG Predicted\" : b})\n",
        "# df1.to_csv(\"/content/drive/MyDrive/ECG_data/original_ECG_1.csv\", index=False)\n",
        "# df2.to_csv(\"/content/drive/MyDrive/ECG_data/generated_ECG_1.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# a = np.array(original_scg)\n",
        "a = np.array(y_test.flatten())\n",
        "b = x_g_AB.flatten()\n",
        "print(len(a), len(b))\n",
        "df1 = pd.DataFrame({ \"ECG Original\" : a})\n",
        "df2 = pd.DataFrame({ \"ECG Predicted\" : b})\n",
        "df1.to_csv(\"original_ECG_10.csv\", index=False)\n",
        "#df2.to_csv(\"/content/drive/MyDrive/ECG_data/generated_ECG_10.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcUbWMkcVTQA"
      },
      "outputs": [],
      "source": [
        "!pip install heartpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpOjSUASVWzH"
      },
      "outputs": [],
      "source": [
        "import heartpy as hp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample_rate = 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ld_q3I2VaDl"
      },
      "outputs": [],
      "source": [
        "data = hp.get_data('/content/original_ECG_10.csv')\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwCoAPRaVgN7"
      },
      "outputs": [],
      "source": [
        "#run analysis\n",
        "wd, m = hp.process(data[1:], sample_rate)\n",
        "\n",
        "#visualise in plot of custom size\n",
        "plt.figure(figsize=(12,4))\n",
        "hp.plotter(wd, m)\n",
        "#print(len(wd['hr']))\n",
        "#print(wd.keys())\n",
        "#print(wd['RR_list'])\n",
        "#heart rates\n",
        "hrs = 60000/wd['RR_list']\n",
        "print(\"heart rates: \",hrs)\n",
        "#display computed measures\n",
        "for measure in m.keys():\n",
        "    print('%s: %f' %(measure, m[measure]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRmQp2UPVjL0"
      },
      "outputs": [],
      "source": [
        "\n",
        "df2 = pd.DataFrame({ \"Heart Rate\" : np.array(hrs)})\n",
        "df2.to_csv(\"hr_original_10.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMobxa04W9_2"
      },
      "outputs": [],
      "source": [
        "data = hp.get_data('original_ECG_15.csv')\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1qofWbnXE6x"
      },
      "outputs": [],
      "source": [
        "#run analysis\n",
        "wd, m = hp.process(data[1:], sample_rate)\n",
        "\n",
        "#visualise in plot of custom size\n",
        "plt.figure(figsize=(12,4))\n",
        "hp.plotter(wd, m)\n",
        "#print(len(wd['hr']))\n",
        "#print(wd.keys())\n",
        "#print(wd['RR_list'])\n",
        "#heart rates\n",
        "hrs = 60000/wd['RR_list']\n",
        "print(\"heart rates: \",hrs)\n",
        "#display computed measures\n",
        "for measure in m.keys():\n",
        "    print('%s: %f' %(measure, m[measure]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGAOERatXImS"
      },
      "outputs": [],
      "source": [
        "\n",
        "df2 = pd.DataFrame({ \"Heart Rate\" : np.array(hrs)})\n",
        "df2.to_csv(\"/content/drive/MyDrive/ECG_data/hr_original_12.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9FkK6KzUdUK"
      },
      "outputs": [],
      "source": [
        "data = hp.get_data('/content/drive/MyDrive/ECG_data/hr_original_10.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md_oDmlrAEfa"
      },
      "outputs": [],
      "source": [
        "for i in range(len(data)):\n",
        "  if data[i]>500:\n",
        "    data[i]=data[i]/10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h-0_RjqAHTX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df2 = pd.DataFrame({ \"Heart Rate\" : np.array(data)})\n",
        "df2.to_csv(\"/content/drive/MyDrive/ECG_data/hr_original_10.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-cQFHJz_Nrf"
      },
      "outputs": [],
      "source": [
        "plt.plot(x_g_AB[1])\n",
        "plt.plot(y_test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PWaTgxKUEQg"
      },
      "outputs": [],
      "source": [
        "#X_test_in = []\n",
        "#x_g_AB_in = []\n",
        "#inverse differencing\n",
        "#for i in range(len(X_test)):\n",
        "#  X_test_in.append((np.concatenate(([X_test[i][0]], X_test[i])).cumsum())[:x_g_AB.shape[1]])\n",
        "#  x_g_AB_in.append((np.concatenate(([x_g_AB[i][0]], x_g_AB[i])).cumsum())[:x_g_AB.shape[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvGRVswC-0aO"
      },
      "outputs": [],
      "source": [
        "len(x_g_AB_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v2SVaik_I2z"
      },
      "outputs": [],
      "source": [
        "len(x_g_AB_in[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8CBh3HxUW1t"
      },
      "outputs": [],
      "source": [
        "# x_g_AB = np.array(x_g_AB_in).reshape(x_g_AB.shape[0],x_g_AB.shape[1],1)\n",
        "# X_test = np.array(X_test_in).reshape(X_test.shape[0],X_test.shape[1],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPHBd5X9VDNy"
      },
      "outputs": [],
      "source": [
        "def smooth(x,window_len=7,window='hanning'):\n",
        "    \"\"\"smooth the data using a window with requested size.\n",
        "\n",
        "    This method is based on the convolution of a scaled window with the signal.\n",
        "    The signal is prepared by introducing reflected copies of the signal\n",
        "    (with the window size) in both ends so that transient parts are minimized\n",
        "    in the begining and end part of the output signal.\n",
        "\n",
        "    input:\n",
        "        x: the input signal\n",
        "        window_len: the dimension of the smoothing window; should be an odd integer\n",
        "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
        "            flat window will produce a moving average smoothing.\n",
        "\n",
        "    output:\n",
        "        the smoothed signal\n",
        "\n",
        "    example:\n",
        "\n",
        "    t=linspace(-2,2,0.1)\n",
        "    x=sin(t)+randn(len(t))*0.1\n",
        "    y=smooth(x)\n",
        "\n",
        "    see also:\n",
        "\n",
        "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
        "    scipy.signal.lfilter\n",
        "\n",
        "    TODO: the window parameter could be the window itself if an array instead of a string\n",
        "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
        "    \"\"\"\n",
        "\n",
        "    if window_len<3:\n",
        "        return x\n",
        "\n",
        "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
        "    #print(len(s))\n",
        "    if window == 'flat': #moving average\n",
        "        w=np.ones(window_len,'d')\n",
        "    else:\n",
        "        w=eval('np.'+window+'(window_len)')\n",
        "\n",
        "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW-8-EYQZZF4"
      },
      "outputs": [],
      "source": [
        "x_g_AB_reshaped = x_g_AB.reshape(x_g_AB.shape[0],x_g_AB.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u163Y4jAZinu"
      },
      "outputs": [],
      "source": [
        "x_g_AB_reshaped.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6zBS52LTMKN"
      },
      "outputs": [],
      "source": [
        "x_g_AB_smoothed = []\n",
        "for i in range(len(x_g_AB_reshaped)):\n",
        "  smoothed = smooth(x_g_AB_reshaped[i])\n",
        "  x_g_AB_smoothed.append(smoothed[:len(x_g_AB_reshaped[i])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrJEw9AjM2eI"
      },
      "outputs": [],
      "source": [
        "#          Testing functions\n",
        "#################################################################################\n",
        "def rmse(targets, predictions):\n",
        "    return np.sqrt(np.mean((targets-predictions)**2))\n",
        "\n",
        "\n",
        "def prd(targets, predictions):\n",
        "    s1 = np.sum((targets-predictions)**2)\n",
        "    s2 = np.sum(targets**2)\n",
        "    return np.sqrt(s1 / s2 * 100)\n",
        "\n",
        "\n",
        "def mmd(targets, predictions):\n",
        "    mmd_stat = MMDStatistic(400, 400)\n",
        "    sample_target = torch.from_numpy(targets.numpy().reshape((400,1)))\n",
        "    sample_pred = torch.from_numpy(predictions.numpy().reshape((400,1)))\n",
        "\n",
        "    stat = mmd_stat(sample_target, sample_pred, [1.])\n",
        "    return(stat.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxYLtf_JYGyZ"
      },
      "outputs": [],
      "source": [
        "x_g_AB_smoothed = np.array(x_g_AB_smoothed)\n",
        "x_g_AB_smoothed = x_g_AB_smoothed.reshape(x_g_AB_smoothed.shape[0],x_g_AB_smoothed.shape[1],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWazhMXzrh8P"
      },
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k8C5LdhrgP0"
      },
      "outputs": [],
      "source": [
        "x_g_AB_smoothed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCtZY8RUQtjf"
      },
      "outputs": [],
      "source": [
        "#for i in range(len(x_g_AB)):\n",
        "rmse_arr = rmse(y_test,x_g_AB)\n",
        "print(rmse_arr)\n",
        "rmse_smoothed = rmse(y_test,x_g_AB_smoothed)\n",
        "print(\"smoothed\",rmse_smoothed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6pV9wzdlTeS"
      },
      "outputs": [],
      "source": [
        "rmse_all =[]\n",
        "rmse_all_smoothened =[]\n",
        "for i in range(len(x_g_AB)):\n",
        "  rmse_all.append(rmse(y_test[i],x_g_AB[i]))\n",
        "  rmse_all_smoothened.append(rmse(y_test[i],x_g_AB_smoothed[i]))\n",
        "\n",
        "rmse_all_mean = np.array(rmse_all).mean()\n",
        "print(rmse_all_mean)\n",
        "rmse_all_smoothed_mean = np.array(rmse_all_smoothened).mean()\n",
        "print(\"smoothed\",rmse_all_smoothed_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wdDoH7eQ6Hh"
      },
      "outputs": [],
      "source": [
        "\n",
        "prd_arr = prd(y_test,x_g_AB)\n",
        "print(prd_arr)\n",
        "prd_arr_smoothed = prd(y_test,x_g_AB_smoothed)\n",
        "print(\"smoothed\",prd_arr_smoothed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHBGblEDuehI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mae_arr = mean_absolute_error(y_test.flatten(),x_g_AB.flatten())\n",
        "print(mae_arr)\n",
        "mae_arr_smoothed = mean_absolute_error(y_test.flatten(),x_g_AB_smoothed.flatten())\n",
        "print(\"smoothed\",mae_arr_smoothed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQVy4CB10gIT"
      },
      "outputs": [],
      "source": [
        "def detect_peaks(ecg_signal, threshold=0.3, qrs_filter=None):\n",
        "    '''\n",
        "    Peak detection algorithm using cross corrrelation and threshold\n",
        "    '''\n",
        "    if qrs_filter is None:\n",
        "        # create default qrs filter, which is just a part of the sine function\n",
        "        t = np.linspace(1.5 * np.pi, 3.5 * np.pi, 15)\n",
        "        qrs_filter = np.sin(t)\n",
        "\n",
        "    # normalize data\n",
        "    ecg_signal = (ecg_signal - ecg_signal.mean()) / ecg_signal.std()\n",
        "\n",
        "    # calculate cross correlation\n",
        "    similarity = np.correlate(ecg_signal, qrs_filter, mode=\"same\")\n",
        "    similarity = similarity / np.max(similarity)\n",
        "\n",
        "    # return peaks (values in ms) using threshold\n",
        "    return ecg_signal[similarity > threshold].index, similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sO0w_0O085l"
      },
      "outputs": [],
      "source": [
        "def group_peaks(p, threshold=5):\n",
        "    '''\n",
        "    The peak detection algorithm finds multiple peaks for each QRS complex.\n",
        "    Here we group collections of peaks that are very near (within threshold) and we take the median index\n",
        "    '''\n",
        "    # initialize output\n",
        "    output = np.empty(0)\n",
        "\n",
        "    # label groups of sample that belong to the same peak\n",
        "    peak_groups, num_groups = label(np.diff(p) < threshold)\n",
        "\n",
        "    # iterate through groups and take the mean as peak index\n",
        "    for i in np.unique(peak_groups)[1:]:\n",
        "        peak_group = p[np.where(peak_groups == i)]\n",
        "        output = np.append(output, np.median(peak_group))\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW16tGB_5H12"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(x_g_AB.flatten(), columns = ['ECG'])\n",
        "df2 = pd.DataFrame(X_test.flatten(), columns = ['SCG'])\n",
        "df3 = pd.DataFrame(y_test.flatten(), columns = ['ECG_orig'])\n",
        "#df['column_name']=pd.Series(x_g_AB.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl2uDfgknydZ"
      },
      "outputs": [],
      "source": [
        "n_samples = f.getNSamples()[0]\n",
        "\n",
        "# Extract the signal data and flatten it\n",
        "signal_data = f.readSignal(0).flatten()\n",
        "\n",
        "# Close the file\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG1RQ-DoOSoy"
      },
      "outputs": [],
      "source": [
        "!pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7cR2ZpfPWQW"
      },
      "outputs": [],
      "source": [
        "import wfdb\n",
        "from wfdb import processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u5zX8_aGcTm"
      },
      "outputs": [],
      "source": [
        "# to be used for wsdb to edf conversion and reading data using wsdb\n",
        "import wfdb\n",
        "from google.colab import files\n",
        "fileName_str\n",
        "record = wfdb.rdrecord('b001', pn_dir='cebsdb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tlWmRfQnsGt"
      },
      "outputs": [],
      "source": [
        "# Get the number of channels and samples\n",
        "n_channels = f.signals_in_file\n",
        "n_samples = f.getNSamples()[0]\n",
        "\n",
        "# Extract the signal data and flatten it\n",
        "signal_data = f.readSignal(0).flatten()\n",
        "\n",
        "# Close the file\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKlovSfviNPG"
      },
      "outputs": [],
      "source": [
        "record.p_signal[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBWEowuanOEA"
      },
      "outputs": [],
      "source": [
        "qrs = codeFn(ecg_signal, fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEjBAFgSPcVf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Demo 19 - Use the GQRS detection algorithm and correct the peaks\n",
        "\n",
        "def peaks_hr(sig, peak_inds, fs, title, figsize=(20, 10), saveto=None):\n",
        "    \"Plot a signal with its peaks and heart rate\"\n",
        "    # Calculate heart rate\n",
        "    hrs = processing.hr.compute_hr(sig_len=sig.shape[0], qrs_inds=peak_inds, fs=fs)\n",
        "    print(hrs)\n",
        "    N = sig.shape[0]\n",
        "\n",
        "    fig, ax_left = plt.subplots(figsize=figsize)\n",
        "    ax_right = ax_left.twinx()\n",
        "\n",
        "    ax_left.plot(sig, color='#3979f0', label='Signal')\n",
        "    ax_left.plot(peak_inds, sig[peak_inds], 'rx', marker='x',\n",
        "                 color='#8b0000', label='Peak', markersize=12)\n",
        "    ax_right.plot(np.arange(N), hrs, label='Heart rate', color='m', linewidth=2)\n",
        "\n",
        "    ax_left.set_title(title)\n",
        "\n",
        "    ax_left.set_xlabel('Time (ms)')\n",
        "    ax_left.set_ylabel('ECG (mV)', color='#3979f0')\n",
        "    ax_right.set_ylabel('Heart rate (bpm)', color='m')\n",
        "    # Make the y-axis label, ticks and tick labels match the line color.\n",
        "    ax_left.tick_params('y', colors='#3979f0')\n",
        "    ax_right.tick_params('y', colors='m')\n",
        "    if saveto is not None:\n",
        "        plt.savefig(saveto, dpi=600)\n",
        "    plt.show()\n",
        "    return hrs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2ZCHPZXoCxJ"
      },
      "outputs": [],
      "source": [
        "t = np.arange(len(ecg_signal)) / fs\n",
        "plt.plot(t, ecg_signal)\n",
        "plt.plot(qrs/fs, ecg_signal[qrs], 'ro')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('ECG Amplitude')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OajguMORasOE"
      },
      "outputs": [],
      "source": [
        "# Load the WFDB record and the physical samples\n",
        "#record = wfdb.rdrecord('sample-data/100', sampfrom=0, sampto=10000, channels=[0])\n",
        "\n",
        "# Use the GQRS algorithm to detect QRS locations in the first channel\n",
        "qrs_inds = processing.qrs.gqrs_detect(sig=record.p_signal[:,0], fs=record.fs)\n",
        "\n",
        "# Plot results\n",
        "hrs = peaks_hr(sig=record.p_signal, peak_inds=qrs_inds, fs=record.fs,\n",
        "         title=\"GQRS peak detection on record b001\")\n",
        "\n",
        "# Correct the peaks shifting them to local maxima\n",
        "min_bpm = 20\n",
        "max_bpm = 230\n",
        "#min_gap = record.fs * 60 / min_bpm\n",
        "# Use the maximum possible bpm as the search radius\n",
        "search_radius = int(record.fs * 60 / max_bpm)\n",
        "corrected_peak_inds = processing.peaks.correct_peaks(record.p_signal[:,0],\n",
        "                                                     peak_inds=qrs_inds,\n",
        "                                                     search_radius=search_radius,\n",
        "                                                     smooth_window_size=150)\n",
        "\n",
        "# Display results\n",
        "print('Corrected GQRS detected peak indices:', sorted(corrected_peak_inds))\n",
        "hrs = peaks_hr(sig=record.p_signal, peak_inds=sorted(corrected_peak_inds), fs=record.fs,\n",
        "         title=\"Corrected GQRS peak detection on sampledata/100\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVKKF0h-LaTR"
      },
      "outputs": [],
      "source": [
        "# Load the WFDB record and the physical samples\n",
        "#record = wfdb.rdrecord('sample-data/100', sampfrom=0, sampto=10000, channels=[0])\n",
        "\n",
        "# Use the GQRS algorithm to detect QRS locations in the first channel\n",
        "qrs_inds = processing.qrs.gqrs_detect(sig=record.p_signal[:,3], fs=record.fs)\n",
        "\n",
        "# Plot results\n",
        "hrs_scg = peaks_hr(sig=record.p_signal, peak_inds=qrs_inds, fs=record.fs,\n",
        "         title=\"GQRS peak detection on record b001\")\n",
        "\n",
        "# Correct the peaks shifting them to local maxima\n",
        "min_bpm = 20\n",
        "max_bpm = 230\n",
        "#min_gap = record.fs * 60 / min_bpm\n",
        "# Use the maximum possible bpm as the search radius\n",
        "search_radius = int(record.fs * 60 / max_bpm)\n",
        "corrected_peak_inds = processing.peaks.correct_peaks(record.p_signal[:,3],\n",
        "                                                     peak_inds=qrs_inds,\n",
        "                                                     search_radius=search_radius,\n",
        "                                                     smooth_window_size=150)\n",
        "\n",
        "# Display results\n",
        "print('Corrected GQRS detected peak indices:', sorted(corrected_peak_inds))\n",
        "hrs_scg = peaks_hr(sig=record.p_signal, peak_inds=sorted(corrected_peak_inds), fs=record.fs,\n",
        "         title=\"Corrected GQRS peak detection on sampledata/100\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyrnU2HEV__r"
      },
      "outputs": [],
      "source": [
        "len(hrs)\n",
        "hrs = hrs[~np.isnan(hrs)]\n",
        "hrs=hrs[hrs<160]\n",
        "#hrs=hrs[hrs>100]\n",
        "print(len(hrs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DhtZiNIWGSP"
      },
      "outputs": [],
      "source": [
        "len(hrs_scg)\n",
        "\n",
        "hrs_scg = hrs_scg[~np.isnan(hrs_scg)]\n",
        "hrs_scg=hrs_scg[hrs_scg<160]\n",
        "#hrs_scg=hrs_scg[hrs_scg>100]\n",
        "print(len(hrs_scg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HsDwqZID_xZ"
      },
      "outputs": [],
      "source": [
        "# plotting the data\n",
        "plt.scatter(hrs[:len(hrs_scg)], hrs_scg)\n",
        "\n",
        "# This will fit the best line into the graph\n",
        "plt.plot(np.unique(hrs), np.poly1d(np.polyfit(hrs[:len(hrs_scg)], hrs_scg, 1))\n",
        "         (np.unique(hrs)), color='red')\n",
        "plt.ylabel(\"SCG heart rate\")\n",
        "plt.xlabel(\"ECG heart rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvedI0cjGOmL"
      },
      "outputs": [],
      "source": [
        "!pip install pyhrv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0PTfIEd1MGa"
      },
      "outputs": [],
      "source": [
        "# detect peaks\n",
        "print(len(df.ECG))\n",
        "print(len(df2.SCG))\n",
        "print(len(df3.ECG_orig))\n",
        "peaks_ecg_pred, similarity = detect_peaks(df.ECG, threshold=0.3)\n",
        "peaks_scg, similarity = detect_peaks(df2.SCG, threshold=0.3)\n",
        "peaks_ecg, similarity = detect_peaks(df3.ECG_orig, threshold=0.3)\n",
        "\n",
        "print(len(peaks_ecg_pred))\n",
        "print(len(peaks_scg))\n",
        "print(len(peaks_ecg))\n",
        "# group peaks so we get a single peak per beat (hopefully)\n",
        "\n",
        "from scipy.ndimage import label\n",
        "grouped_peaks_ecg_pred = group_peaks(peaks_ecg_pred)\n",
        "grouped_peaks_scg = group_peaks(peaks_scg)\n",
        "grouped_peaks_ecg = group_peaks(peaks_ecg)\n",
        "\n",
        "print(len(grouped_peaks_ecg_pred))\n",
        "print(len(grouped_peaks_scg))\n",
        "print(len(grouped_peaks_ecg))\n",
        "# RR-intervals are the differences between successive peaks\n",
        "rr_ecg_pred = np.diff(grouped_peaks_ecg_pred)\n",
        "rr_scg = np.diff(grouped_peaks_scg)\n",
        "rr_ecg = np.diff(grouped_peaks_ecg)\n",
        "\n",
        "print(len(rr_ecg_pred))\n",
        "print(len(rr_scg))\n",
        "print(len(rr_ecg))\n",
        "# plot RR-intervals\n",
        "#plt.figure(figsize=(20, 7))\n",
        "#plt.title(\"RR-intervals\")\n",
        "#plt.xlabel(\"Time (ms)\")\n",
        "#plt.ylabel(\"RR-interval (ms)\")\n",
        "\n",
        "#plt.plot(np.cumsum(rr), rr, label=\"RR-interval\", color=\"#A651D8\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zdicn0_cI5tj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM98N4hTIhbL"
      },
      "outputs": [],
      "source": [
        "def plot_poincare(rr):\n",
        "    rr_n = rr[:-1]\n",
        "    rr_n1 = rr[1:]\n",
        "\n",
        "    sd1 = np.sqrt(0.5) * np.std(rr_n1 - rr_n)\n",
        "    sd2 = np.sqrt(0.5) * np.std(rr_n1 + rr_n)\n",
        "\n",
        "    m = np.mean(rr)\n",
        "    min_rr = np.min(rr)\n",
        "    max_rr = np.max(rr)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.title(\"Poincare plot\")\n",
        "\n",
        "    sns.scatterplot(x=rr_n, y=rr_n1, color=\"#51A6D8\")\n",
        "\n",
        "    plt.xlabel(r'$RR_n (ms)$')\n",
        "    plt.ylabel(r'$RR_{n+1} (ms)$')\n",
        "\n",
        "    e1 = Ellipse((m, m), 2*sd1, 2*sd2, angle=-45, linewidth=1.2, fill=False, color=\"k\")\n",
        "    plt.gca().add_patch(e1)\n",
        "\n",
        "    plt.arrow(m, m, (max_rr-min_rr)*0.4, (max_rr-min_rr)*0.4, color=\"k\", linewidth=0.8, head_width=5, head_length=5)\n",
        "    plt.arrow(m, m, (min_rr-max_rr)*0.4, (max_rr-min_rr)*0.4, color=\"k\", linewidth=0.8, head_width=5, head_length=5)\n",
        "\n",
        "    plt.arrow(m, m, sd2 * np.sqrt(0.5), sd2 * np.sqrt(0.5), color=\"green\", linewidth=5)\n",
        "    plt.arrow(m, m, -sd1 * np.sqrt(0.5), sd1 * np.sqrt(0.5), color=\"red\", linewidth=5)\n",
        "\n",
        "    plt.text(max_rr, max_rr, \"SD2\", fontsize=20, color=\"green\")\n",
        "    plt.text(m-(max_rr-min_rr)*0.4-20, max_rr, \"SD1\", fontsize=20, color=\"red\")\n",
        "\n",
        "    return sd1, sd2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XtMdphUInOn"
      },
      "outputs": [],
      "source": [
        "sd1, sd2 = plot_poincare(rr_ecg_pred)\n",
        "print(\"SD1: %.3f ms\" % sd1)\n",
        "print(\"SD2: %.3f ms\" % sd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9RogA2aBkBl"
      },
      "outputs": [],
      "source": [
        "sd1, sd2 = plot_poincare(rr_scg)\n",
        "print(\"SD1: %.3f ms\" % sd1)\n",
        "print(\"SD2: %.3f ms\" % sd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcMUKpkBGcVP"
      },
      "outputs": [],
      "source": [
        "import pyhrv\n",
        "pyhrv.nonlinear.poincare(rr_ecg_pred,peaks_ecg_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq7uWKlkv5YL"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "true = y_test.astype('float')\n",
        "pre =  x_g_AB.astype('float')\n",
        "corr, pval =pearsonr(true.flatten(),pre.flatten())\n",
        "print(\"corre and p-val:: \",corr, pval)\n",
        "\n",
        "\n",
        "pre_smoothed =  x_g_AB_smoothed.astype('float')\n",
        "corr_sm, pval_sm =pearsonr(true.flatten(),pre_smoothed.flatten())\n",
        "print(\"smoothed corre and p-val:: \",corr_sm, pval_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzA3hVXVKqvG"
      },
      "outputs": [],
      "source": [
        "accuracy_for_AtoB = 100 - (np.mean(np.abs(x_g_AB-y_test)*100))\n",
        "print(accuracy_for_AtoB)\n",
        "\n",
        "accuracy_for_AtoB_sm = 100 - (np.mean(np.abs(x_g_AB_smoothed-y_test)*100))\n",
        "print(\"smoothed\",accuracy_for_AtoB_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zJcNkEDIeWj"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.dpi'] = 300\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiGoXcO2HLNl"
      },
      "outputs": [],
      "source": [
        "print(len(str(fileNames[0]))-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rz4ZFn5UxcS"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "fig.set_size_inches(50,7)\n",
        "plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "plt.plot(np.array(x_g_AB).flatten()[:5000], color=\"red\")\n",
        "plt.show()\n",
        "fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_first_5000.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqQmbihI3VFj"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "fig.set_size_inches(50,7)\n",
        "plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "# plt.plot(np.array(x_g_AB).flatten()[:5000], color=\"red\")\n",
        "plt.show()\n",
        "fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_blue.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nOF_7Vg5p6z"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "fig.set_size_inches(50,7)\n",
        "# plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "plt.plot(np.array(x_g_AB).flatten()[:5000], color=\"red\")\n",
        "plt.show()\n",
        "fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_red.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccpSirZgbL7O"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "fig.set_size_inches(50,7)\n",
        "plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "plt.plot(np.array(x_g_AB_smoothed).flatten()[:5000], color=\"red\")\n",
        "plt.show()\n",
        "fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_first_5000_smoothed.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9BydaDI51Dr"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "fig.set_size_inches(50,7)\n",
        "plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "# plt.plot(np.array(x_g_AB_smoothed).flatten()[:5000], color=\"red\")\n",
        "plt.show()\n",
        "fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_blue_smoothed.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwXrm5MH6Idu"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "fig.set_size_inches(50,7)\n",
        "# plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "plt.plot(np.array(x_g_AB_smoothed).flatten()[:5000], color=\"red\")\n",
        "plt.show()\n",
        "fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_red_smoothed.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSjZzZUsVD_x"
      },
      "outputs": [],
      "source": [
        "# for i in range(len(y_test)):\n",
        "#   fig = plt.figure(figsize=(4,3))\n",
        "#   fig.set_size_inches(50,7)\n",
        "#   plt.plot(y_test[i],color=\"blue\")\n",
        "#   plt.plot(x_g_AB[i], color=\"red\")\n",
        "#   plt.show()\n",
        "#   plt.savefig('/content/drive/MyDrive/Colab Notebooks/GAN_PAPER_DATA/basal/b_to_b_1_batch_'+str(i+1)+'.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mb0GpQobaAR"
      },
      "outputs": [],
      "source": [
        "# for i in range(len(y_test)):\n",
        "#   fig = plt.figure(figsize=(4,3))\n",
        "#   fig.set_size_inches(50,7)\n",
        "#   plt.plot(y_test[i],color=\"blue\")\n",
        "#   plt.plot(x_g_AB_smoothed[i], color=\"red\")\n",
        "#   plt.savefig('/content/drive/MyDrive/Colab Notebooks/GAN_PAPER_DATA/basal/b_to_b_1_smoothed_batch_'+str(i+1)+'.jpg')\n",
        "#   plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R4embaispm9"
      },
      "outputs": [],
      "source": [
        "# !pip install frechetdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsD1i3QNcs_a"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# print(sys.getrecursionlimit())\n",
        "# sys.setrecursionlimit(5000)\n",
        "# print(sys.getrecursionlimit())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtiJpcXEnjc6"
      },
      "outputs": [],
      "source": [
        "# from frechetdist import frdist\n",
        "# fd = []\n",
        "# for i in range(len(y_test)):\n",
        "#   fd.append(frdist(y_test[i],x_g_AB[i]))\n",
        "# print(\"FD: \", np.array(fd).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mapGaw0HvhHa"
      },
      "outputs": [],
      "source": [
        "\n",
        "#mmd_arr = mmd(y_test,x_g_AB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FME3FqsWtliD"
      },
      "outputs": [],
      "source": [
        "# fd_mean = np.array(fd).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqyCpTSVntXV"
      },
      "outputs": [],
      "source": [
        "#To create the blank csv with header.\n",
        "\n",
        "#from csv import writer\n",
        "\n",
        "#List=[\"Subject\",\"Generator param A\", \"Generator param B\",\"Reconstructor param A\",\"Reconstructor param B\",\"RMSE flattened\", \"RMSE Smoothed flattened\",\"RMSE All Mean\", \"RMSE All Smoothed Mean\", \"prd\", \"prd smoothed\",\"mae\",\"mae smoothed\", \"correlation\",\"correlation smoothed\",\"p-val\",\"p-val smoothed\",\"accuracy\",\"accuracy smoothed\",\"frachet dist\"]\n",
        "\n",
        "#with open('/content/drive/MyDrive/Colab Notebooks/GAN_PAPER_DATA/basal/matrices.csv', 'w') as f_object:\n",
        "\n",
        "#    writer_object = writer(f_object)\n",
        "#    writer_object.writerow(List)\n",
        "\n",
        "#    f_object.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tBmFw4I12-y"
      },
      "outputs": [],
      "source": [
        "# change the parameters here\n",
        "gen_param_a = 0\n",
        "gen_param_b = 0\n",
        "rec_param_a = 0\n",
        "rec_param_b = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW92FBJMs1rq"
      },
      "outputs": [],
      "source": [
        "# List\n",
        "List_vals =[fileNames[0], gen_param_a,gen_param_b,rec_param_a,rec_param_b, rmse_arr, rmse_smoothed, rmse_all_mean, rmse_all_smoothed_mean, prd_arr, prd_arr_smoothed,mae_arr,mae_arr_smoothed, corr,corr_sm,pval,pval_sm,accuracy_for_AtoB,accuracy_for_AtoB_sm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYk_0-G-t3sX"
      },
      "outputs": [],
      "source": [
        "\n",
        "from csv import writer\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/basal/matrices.csv', 'a') as f_object:\n",
        "\n",
        "    writer_object = writer(f_object)\n",
        "    writer_object.writerow(List_vals)\n",
        "\n",
        "    f_object.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSqpdHYCJA8y"
      },
      "outputs": [],
      "source": [
        "# !pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_Xg6pwAiY4h"
      },
      "outputs": [],
      "source": [
        "# !pip install -c dloewenstein torch-two-sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aH-uFRmJZm5"
      },
      "outputs": [],
      "source": [
        "#!pip install mmd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q90eindpKZ7G"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S733gBm7Qrx5"
      },
      "outputs": [],
      "source": [
        "# from torch_two_sample.statistics_diff import MMDStatistic\n",
        "# print(mmd(y_test,x_g_AB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X75GyDycKd0j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV3DzxzNGsdJ"
      },
      "outputs": [],
      "source": [
        "#!pip install ecg_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPx2jXZawBbP"
      },
      "outputs": [],
      "source": [
        "#import ecg_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnC5ZOPVo6yi"
      },
      "outputs": [],
      "source": [
        "#for i in range(10):\n",
        "#  ecg_plot.plot_1(y_test_scaled[i], sample_rate=500, title = 'Original ECG')\n",
        " # ecg_plot.plot_1(x_g_AB_scaled[i], sample_rate=500, title = 'Predicted ECG')\n",
        "  #plt.tight_layout()\n",
        " # ecg_plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt7WsGomHyAD"
      },
      "outputs": [],
      "source": [
        "\n",
        "#ecg_plot.plot_1(y_test.flatten(), sample_rate=1000, title = 'Original')\n",
        "#ecg_plot.plot_1(x_g_AB.flatten(), sample_rate=1000, title = 'Predicted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNUu09O8WhX3"
      },
      "outputs": [],
      "source": [
        "#x = y_test[0].flatten().tolist()\n",
        "#y = x_g_AB[0].flatten().tolist()\n",
        "#for i in range(len(x_g_AB)):\n",
        "#  x.append((y_test[i][len(y_test[i])-1])[0])\n",
        "#  y.append((x_g_AB[i][len(x_g_AB[i])-1])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr7ofa2xOc49"
      },
      "outputs": [],
      "source": [
        "print(\"acc: %3d%%\"% accuracy_for_AtoB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux9A5BOdPNZD"
      },
      "outputs": [],
      "source": [
        "# normalize correlation\n",
        "a = (y_test - np.mean(y_test)) / (np.std(y_test) * len(y_test))\n",
        "b = (x_g_AB - np.mean(x_g_AB)) / (np.std(x_g_AB))\n",
        "c = np.correlate(a.flatten(), b.flatten(), 'full')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVfrUH2l_X3o"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test.flatten(), x_g_AB.flatten())\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SuBwpYd_oO_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mae = mean_absolute_error(y_test.flatten(), x_g_AB.flatten())\n",
        "print(mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE1VHpvD0-AH"
      },
      "outputs": [],
      "source": [
        "y_pred = x_g_AB.flatten()\n",
        "y_test_new = y_test.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs64HmthxRFs"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred[y_pred > 0.5] = 1\n",
        "y_pred[y_pred <= 0.5] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn6Cv1P9xo49"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_test_new[y_test_new > 0.5] = 1\n",
        "y_test_new[y_test_new <= 0.5] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2cFEBwO_8Wo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score,recall_score,precision_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ailUHhBooDXO"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"accuracy_score\",accuracy_score(y_test_new, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwTBJos0zUAz"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"precision_score\",precision_score(y_test_new, y_pred , pos_label='positive', average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUaN4GV2xig6"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"f1_score\",f1_score(y_test_new, y_pred, pos_label='positive', average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxidi8ZXxlpA"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"recall_score\",recall_score(y_test_new, y_pred, pos_label='positive', average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAC5ojYUxnmy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "a = [1,1,1,1,1,0]\n",
        "b=[1,1,1,1,0,0]\n",
        "matrix = confusion_matrix(y_test_new, y_pred)\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BIF5Y8gC2-v"
      },
      "outputs": [],
      "source": [
        "# correlation coefficient r of x and y\n",
        "from scipy import stats\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(y_test_new, y_pred)\n",
        "print (r_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnqyx3KsDERn"
      },
      "outputs": [],
      "source": [
        "# The coefficient of determination\n",
        "from sklearn.metrics import r2_score\n",
        "print(r2_score(y_test_new, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQrVXwacCWVj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}